{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This notebook sumarrizes character/word error rates (with 95% CIs) across all held-out data for both train/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "\n",
    "#point this towards the top level dataset directory\n",
    "rootDir = os.path.expanduser('~') + '/handwritingBCIData/'\n",
    "\n",
    "#point this towards the code directory\n",
    "repoDir = os.getcwd() + '/'\n",
    "\n",
    "#defines which datasets to process\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "#summarize performance for both train/test partitions and for three versions: \n",
    "#'Raw' (no language model), 'Bigram LM' (kaldi bigram language model only), and 'Bigram LM + GPT-2 Rescore' (kaldi model + GPT-2)\n",
    "cvParts = ['HeldOutBlocks', 'HeldOutTrials']\n",
    "resultsDir = ['RNNTrainingSteps/Step5_RNNInference','RNNTrainingSteps/Step6_ApplyBigramLM','RNNTrainingSteps/Step7_GPT2Rescore']\n",
    "resultsNames = ['Raw','Bigram LM','Bigram LM + GPT-2 Rescore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeldOutBlocks - Raw\n",
      "Character error rate: 5.32% (95% CI = [4.81, 5.86])\n",
      "Word error rate:      23.28% (95% CI = [21.27, 25.41])\n",
      " \n",
      "HeldOutBlocks - Bigram LM\n",
      "Character error rate: 1.69% (95% CI = [1.32, 2.10])\n",
      "Word error rate:      6.10% (95% CI = [4.97, 7.25])\n",
      " \n",
      "HeldOutBlocks - Bigram LM + GPT-2 Rescore\n",
      "Character error rate: 0.90% (95% CI = [0.62, 1.23])\n",
      "Word error rate:      3.21% (95% CI = [2.37, 4.11])\n",
      " \n",
      "HeldOutTrials - Raw\n",
      "Character error rate: 2.78% (95% CI = [2.20, 3.41])\n",
      "Word error rate:      12.88% (95% CI = [10.28, 15.63])\n",
      " \n",
      "HeldOutTrials - Bigram LM\n",
      "Character error rate: 0.80% (95% CI = [0.44, 1.22])\n",
      "Word error rate:      3.64% (95% CI = [2.11, 5.34])\n",
      " \n",
      "HeldOutTrials - Bigram LM + GPT-2 Rescore\n",
      "Character error rate: 0.34% (95% CI = [0.14, 0.61])\n",
      "Word error rate:      1.97% (95% CI = [0.78, 3.41])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Summarize character error rate and word error rate across all sessions\n",
    "for cvPart in cvParts:\n",
    "    for resultIdx in range(len(resultsDir)):\n",
    "        allErrCounts = []\n",
    "\n",
    "        for dataDir in dataDirs:\n",
    "            filePath = rootDir + resultsDir[resultIdx] + '/' + cvPart + '/' + dataDir + '_errCounts.mat'\n",
    "            if not os.path.isfile(filePath):\n",
    "                continue\n",
    "            \n",
    "            dat = scipy.io.loadmat(filePath)\n",
    "            cvPartFile = scipy.io.loadmat(rootDir+'RNNTrainingSteps/trainTestPartitions_'+cvPart+'.mat')\n",
    "            valIdx = cvPartFile[dataDir+'_test']\n",
    "\n",
    "            if len(valIdx)==0:\n",
    "                continue\n",
    "\n",
    "            valIdx = valIdx[0,:]\n",
    "            allErrCounts.append(np.stack([dat['charCounts'][0,valIdx],\n",
    "                                 dat['charErrors'][0,valIdx],\n",
    "                                 dat['wordCounts'][0,valIdx],\n",
    "                                 dat['wordErrors'][0,valIdx]],axis=0).T)\n",
    "\n",
    "        if allErrCounts==[]:\n",
    "            continue\n",
    "            \n",
    "        concatErrCounts = np.squeeze(np.concatenate(allErrCounts, axis=0))\n",
    "        cer = 100*(np.sum(concatErrCounts[:,1]) / np.sum(concatErrCounts[:,0]))\n",
    "        wer = 100*(np.sum(concatErrCounts[:,3]) / np.sum(concatErrCounts[:,2]))\n",
    "        \n",
    "        #compute 95% CI using bootstrap resampling\n",
    "        nResamples = 10000\n",
    "        resampledRates = np.zeros([nResamples,2])\n",
    "        for n in range(nResamples):\n",
    "            resampleIdx = np.random.randint(0,concatErrCounts.shape[0],[concatErrCounts.shape[0]])\n",
    "            resampledRates[n,0] = 100*(np.sum(concatErrCounts[resampleIdx,1]) / np.sum(concatErrCounts[resampleIdx,0]))\n",
    "            resampledRates[n,1] = 100*(np.sum(concatErrCounts[resampleIdx,3]) / np.sum(concatErrCounts[resampleIdx,2]))\n",
    "\n",
    "        charCI = np.percentile(resampledRates[:,0],[2.5, 97.5])\n",
    "        wordCI = np.percentile(resampledRates[:,1],[2.5, 97.5])\n",
    "        \n",
    "        print(cvPart + ' - ' + resultsNames[resultIdx])\n",
    "        print('Character error rate: %1.2f%% (95%% CI = [%1.2f, %1.2f])' % (float(cer), float(charCI[0]), float(charCI[1])))\n",
    "        print('Word error rate:      %1.2f%% (95%% CI = [%1.2f, %1.2f])' % (float(wer), float(wordCI[0]), float(wordCI[1])))\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
